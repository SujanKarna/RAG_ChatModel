{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP51cqNHGH1mv+95MptUQIL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SujanKarna/RAG_ChatModel/blob/master/Langchain_chatModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1d59RYWZHmTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import openai\n",
        "import os"
      ],
      "metadata": {
        "id": "BwoApNjuL3Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiaize the ChromaDb Client\n",
        "client = chromadb.Client(Settings(persist_directory=\"./rag_store\"))"
      ],
      "metadata": {
        "id": "iWIpefO8MR_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a collection\n",
        "collection = client.get_or_create_collection(name=\"rag_docs\")"
      ],
      "metadata": {
        "id": "HObxuYHcNZDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Documents\n",
        "documents = [\n",
        "    \"Retrieval-Augmented Generation (RAG) combines search and generation.\",\n",
        "    \"FAISS is a fast similarity search library developed by Facebook.\",\n",
        "    \"ChromaDB supports metadata filtering and persistent storage.\",\n",
        "    \"SentenceTransformers can convert text into semantic embeddings.\"\n",
        "]"
      ],
      "metadata": {
        "id": "Ew6tsgWUNDXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedd Documents\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "doc_embeddings = embedder.encode(documents).tolist()"
      ],
      "metadata": {
        "id": "rjvxcxk-NqUY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Documents to ChromaDB\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    embeddings=doc_embeddings,\n",
        "    ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
        ")"
      ],
      "metadata": {
        "id": "FsOOWLvWNzpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve top-K relevant documents\n",
        "def retrieve(query, k=3):\n",
        "    query_embedding = embedder.encode([query]).tolist()[0]\n",
        "    results = collection.query(query_embeddings = [query_embedding], n_results = k)\n",
        "    return results['documents'][0]"
      ],
      "metadata": {
        "id": "EtWxN93ZOITT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Open-Source LLMs - Mistral"
      ],
      "metadata": {
        "id": "BX8zrJXiAjY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using local llm from huggingface\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "import torch\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "QGeLDvL0RbIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate answer with llm model\n",
        "# Load Mistral model for generation\n",
        "\n",
        "login(token= userdata.get(\"HUGGINGFACE_TOKEN\"))\n",
        "gen_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Format prompt\n",
        "def generate_answer_local(context, query):\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "              Use the following context to answer the question.\n",
        "              ### Context:{context}\n",
        "              ### Question: {query}\n",
        "              ### Answer:\"\"\"\n",
        "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\").to(gen_model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = gen_model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7, top_p=0.9)\n",
        "    return gen_tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Answer:\")[-1].strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "v5MNpzzyPb-h",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\\n\".join(retrieve(\"What is RAG and how does it work?\"))\n",
        "answer = generate_answer_local(context, \"What is RAG and how does it work?\")\n",
        "print(\"üîç Answer:\\n\", answer)"
      ],
      "metadata": {
        "id": "mi_xC1ObB8pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For openai API"
      ],
      "metadata": {
        "id": "YiCxJ7g4AOcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "5-Y6JB5E8GGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade openai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "M_VIiuWR9RUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Securely retrieve API key from Colab Secrets\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "# üßæ Generate answer using OpenAI GPT\n",
        "def generate_answer(query):\n",
        "    retrieved_docs = retrieve(query)\n",
        "    context = \"\\n\".join(retrieved_docs)\n",
        "    prompt = f\"Use the following context to answer the question:\\n{context}\\n\\nQuestion: {query}\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n"
      ],
      "metadata": {
        "id": "mvcRHtr42IUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedd the query\n",
        "query = \"What is RAG and how does it work?\"\n",
        "answer = generate_answer(query)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "id": "mjZLSadK7Hi7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}