{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7MaQpHwDu24RJpT+1jjk0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SujanKarna/RAG_ChatModel/blob/master/Langchain_chatModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install chromadb sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1d59RYWZHmTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings"
      ],
      "metadata": {
        "id": "BwoApNjuL3Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiaize the ChromaDb Client\n",
        "client = chromadb.Client(Settings(persist_directory=\"./rag_store\"))"
      ],
      "metadata": {
        "id": "iWIpefO8MR_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a collection\n",
        "collection = client.get_or_create_collection(name=\"rag_docs\")"
      ],
      "metadata": {
        "id": "HObxuYHcNZDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Documents\n",
        "documents = [\n",
        "    \"Retrieval-Augmented Generation (RAG) combines search and generation.\",\n",
        "    \"FAISS is a fast similarity search library developed by Facebook.\",\n",
        "    \"ChromaDB supports metadata filtering and persistent storage.\",\n",
        "    \"SentenceTransformers can convert text into semantic embeddings.\"\n",
        "]"
      ],
      "metadata": {
        "id": "Ew6tsgWUNDXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedd Documents\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "doc_embeddings = embedder.encode(documents).tolist()"
      ],
      "metadata": {
        "id": "rjvxcxk-NqUY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Documents to ChromaDB\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    embeddings=doc_embeddings,\n",
        "    ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
        ")"
      ],
      "metadata": {
        "id": "FsOOWLvWNzpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedd the query\n",
        "query = \"What is RAG and how does it work?\"\n",
        "query_embedding = embedder.encode([query]).tolist()[0]"
      ],
      "metadata": {
        "id": "J5qvFsq7N-4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve top-K relevant documents\n",
        "results = collection.query(\n",
        "    query_embeddings=[query_embedding],\n",
        "    n_results=3\n",
        ")\n",
        "retrieved_docs = results['documents'][0]"
      ],
      "metadata": {
        "id": "EtWxN93ZOITT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the retrieved documents\n",
        "retrieved_docs"
      ],
      "metadata": {
        "id": "FALyeSCPOkdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble Context\n",
        "context = \"\\n\".join(retrieved_docs)\n",
        "print(context)"
      ],
      "metadata": {
        "id": "fVwTJ3LSOsp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "import torch\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "QGeLDvL0RbIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate answer with llm model\n",
        "# Load Mistral model for generation\n",
        "\n",
        "login(token=\"\")\n",
        "gen_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# Format prompt\n",
        "prompt = f\"\"\"### Instruction:\n",
        "Use the following context to answer the question.\n",
        "\n",
        "### Context:\n",
        "{context}\n",
        "\n",
        "### Question:\n",
        "{query}\n",
        "\n",
        "### Answer:\"\"\"\n",
        "\n",
        "# Generate response\n",
        "inputs = gen_tokenizer(prompt, return_tensors=\"pt\").to(gen_model.device)\n",
        "with torch.no_grad():\n",
        "    outputs = gen_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "answer = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"üîç Answer:\\n\", answer.split(\"### Answer:\")[-1].strip())"
      ],
      "metadata": {
        "id": "v5MNpzzyPb-h",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}